{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.special import softmax\n",
    "from scipy.stats import entropy\n",
    "from scipy.stats import dirichlet\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeModel:\n",
    "    \"\"\"\n",
    "    Generative model class, this class represents an agent, it will observe outcomes and generate actions.\n",
    "    \"\"\"\n",
    "    def __init__(self, size=None, starting_A_alphas=None, starting_B_alphas=None,\n",
    "                 expected_outcomes=None, starting_state=None):\n",
    "        \"\"\"\n",
    "        :param size: Number of possible states the agent can be in\n",
    "        :param starting_A_alphas: 2-dimensional list of alphas that inform dirichlet distributions from which\n",
    "                                    priors over outcomes given states are generated\n",
    "        :param starting_B_alphas: 3-dimensional list of alphas that inform dirichlet distributions from which\n",
    "                                    priors over states given action and states are generated.\n",
    "        :param expected_outcomes: List specifying the expected outcomes of this agent\n",
    "        :param starting_state: Which state(s) the agent thinks it is in at the start of its lifespan\n",
    "        \"\"\"\n",
    "\n",
    "        if size is None:\n",
    "            size = 5\n",
    "\n",
    "        if starting_A_alphas is None:\n",
    "            starting_A_alphas = np.ones((size, 5))\n",
    "        predicted_outcomes_given_state = np.zeros((size, size))\n",
    "\n",
    "        # Generate probability distributions\n",
    "        for s in range(size):\n",
    "            predicted_outcomes_given_state[s] = dirichlet.mean(starting_A_alphas[s])\n",
    "\n",
    "        if starting_B_alphas is None:\n",
    "            starting_B_alphas = np.ones((3, size, size))\n",
    "        predicted_future_states_given_previous_state_and_action = np.zeros((3, size, size))\n",
    "\n",
    "        # Generate probability distributions\n",
    "        for a in range(starting_B_alphas.shape[0]):\n",
    "            for s in range(starting_B_alphas.shape[1]):\n",
    "                predicted_future_states_given_previous_state_and_action[a][s] = dirichlet.mean(starting_B_alphas[a][s])\n",
    "\n",
    "        if expected_outcomes is None:\n",
    "            expected_outcomes = [0.2] * 5\n",
    "\n",
    "        if starting_state is None:\n",
    "            starting_state = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "\n",
    "        self.A_alphas = starting_A_alphas.copy()  # The A alphas inform the predicted outcomes given a state\n",
    "        self.B_alphas = starting_B_alphas.copy()  # The B alphas inform the predicted state given an action and state\n",
    "        self.starting_A_alphas = starting_A_alphas\n",
    "        self.starting_B_alphas = starting_B_alphas\n",
    "\n",
    "        self.predicted_outcomes_given_state = predicted_outcomes_given_state  # A: Predicted outcomes given the state\n",
    "        self.predicted_future_states_given_previous_state_and_action = predicted_future_states_given_previous_state_and_action  # B: Predicted future state given the current state and the control states(?)\n",
    "        self.expected_outcomes = np.array(expected_outcomes)  # C: Preferred outcomes given the model\n",
    "        self.current_state = starting_state  # D: Starting state given the model\n",
    "        self.Actions = [0, 1, 2]\n",
    "\n",
    "        # Remember which action was taken last timestep, this is important for learning the state transition function\n",
    "        self.action_taken = 0\n",
    "\n",
    "    # Actions\n",
    "    def act(self):\n",
    "        \"\"\"\n",
    "        Generate an action\n",
    "        :return: The action the agent chooses\n",
    "        \"\"\"\n",
    "        action_qualities = []\n",
    "        for action in self.Actions:\n",
    "            # Determine predicted future state based on this action and inferred current state\n",
    "            predicted_state = np.dot(self.current_state,\n",
    "                                     self.predicted_future_states_given_previous_state_and_action[action])\n",
    "            # Determine action quality\n",
    "            predicted_quality = self.determine_quality(predicted_state, self.predicted_outcomes_given_state,\n",
    "                                                       self.expected_outcomes)\n",
    "            action_qualities.append(predicted_quality)\n",
    "\n",
    "        action_probabilities = softmax(np.array(action_qualities))  # Change action qualities into probabilities\n",
    "        best_action = np.random.choice(self.Actions, p=action_probabilities)\n",
    "        self.action_taken = best_action  # Remember chosen action\n",
    "        return best_action\n",
    "\n",
    "    # Observations\n",
    "    def observe(self, outcome):\n",
    "        \"\"\"\n",
    "        Observe the outcome\n",
    "        :param outcome: The outcome an agent observes\n",
    "        :return: The state an agent infers it is in and the surprise (prediction error) the agent experienced at the\n",
    "                    observation, these are for data storing purposes mainly\n",
    "        \"\"\"\n",
    "        # Infer state of the agent\n",
    "        predicted_state = self.aproximate_bayesian_inference(outcome)\n",
    "        number_of_states = len(predicted_state)\n",
    "\n",
    "        surprise = entropy(outcome, np.dot(predicted_state, self.predicted_outcomes_given_state))\n",
    "\n",
    "        # Update distribution parameters based on outcome and inferred state, weighted by the surprise\n",
    "        self.A_alphas += np.multiply(np.resize(outcome, (5, 5)).T, predicted_state).T * surprise\n",
    "        self.B_alphas[self.action_taken] += (np.resize(self.current_state, (\n",
    "        number_of_states, number_of_states)).T * predicted_state) * surprise\n",
    "\n",
    "        # Update P(S|O) and P(S_t| A_{t-1}, S_{t-1})\n",
    "        for s in range(number_of_states):\n",
    "            self.predicted_future_states_given_previous_state_and_action[self.action_taken][s] = dirichlet.mean(\n",
    "                self.B_alphas[self.action_taken][s])\n",
    "            self.predicted_outcomes_given_state[s] = dirichlet.mean(self.A_alphas[s])\n",
    "\n",
    "        # Update current state of the agent\n",
    "        self.current_state = predicted_state\n",
    "        return predicted_state, surprise\n",
    "\n",
    "    def determine_quality(self, pred_state, pred_outcome_given_state, expected_outcome):\n",
    "        \"\"\"\n",
    "        Determine the quality of a predicted state\n",
    "        :param pred_state: Predicted state\n",
    "        :param pred_outcome_given_state: Predicted outcomes given a state\n",
    "        :param expected_outcome: Expected outcomes\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pred_state = np.array(pred_state)\n",
    "        pred_outcome_given_state = np.array(pred_outcome_given_state)\n",
    "        expected_outcome = np.array(expected_outcome)\n",
    "\n",
    "        # Determine predicted outcome\n",
    "        pred_outcome = np.dot(pred_state, pred_outcome_given_state)\n",
    "\n",
    "        # Calculate predicted uncertainty as the expectation\n",
    "        # of the entropy of the outcome, weighted by the\n",
    "        # probability of that outcome\n",
    "        pred_ent = np.sum(pred_state * entropy(pred_outcome_given_state, axis=1))\n",
    "\n",
    "        # Calculate predicted divergence as the Kullback-Leibler\n",
    "        # divergence between the predicted outcome and the expected outcome\n",
    "        pred_div = entropy(pk=pred_outcome, qk=expected_outcome)\n",
    "\n",
    "        # Return the sum of the negatives of these two\n",
    "        return -pred_ent - pred_div\n",
    "\n",
    "    def aproximate_bayesian_inference(self, observed_outcome):\n",
    "        \"\"\"\n",
    "        Infer the state of the agent using an approximation of bayesian inference\n",
    "        :param observed_outcome: The outcome observed by the agent\n",
    "        :return: A probability distribution over states\n",
    "        \"\"\"\n",
    "        observed_outcome = np.array(observed_outcome)\n",
    "        expected_outcomes_given_state = np.array(self.predicted_outcomes_given_state)\n",
    "        previous_action = self.action_taken\n",
    "        previous_state = np.array(self.current_state)\n",
    "        expected_future_states_given_control = np.array(self.predicted_future_states_given_previous_state_and_action)\n",
    "\n",
    "        inferred_state = softmax(np.log(np.dot(expected_outcomes_given_state, observed_outcome)) + np.log(\n",
    "            np.dot(previous_state, expected_future_states_given_control[previous_action])))\n",
    "        return inferred_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeProcess:\n",
    "    \"\"\"\n",
    "    Generative process class, responsible for generating observations and conforming to agent actions\n",
    "    \"\"\"\n",
    "    def __init__(self, nr_of_possible_states=None, nr_of_possible_outcomes=None,\n",
    "                 p_outcomes_given_state=None,\n",
    "                 p_states_given_action_and_state=None, starting_states=None,\n",
    "                 nr_of_agents=1):\n",
    "        \"\"\"\n",
    "        Initialization function for the generative process\n",
    "        :param nr_of_possible_states: Number of possible states an agent can be in\n",
    "        :param nr_of_possible_outcomes: Number of possible outcomes an agent can observe\n",
    "        :param p_outcomes_given_state: Probability of each outcome given a state\n",
    "        :param p_states_given_action_and_state: Probability of each state given an action and state\n",
    "        :param starting_states: True state in which each agent starts\n",
    "        :param nr_of_agents: Number of agents\n",
    "        \"\"\"\n",
    "        if nr_of_possible_states is None:\n",
    "            nr_of_possible_states = 5\n",
    "\n",
    "        if nr_of_possible_outcomes is None:\n",
    "            nr_of_possible_outcomes = 5\n",
    "\n",
    "        if p_states_given_action_and_state is None:\n",
    "            p_states_given_action_and_state = self.create_transition_matrix(nr_of_possible_states)\n",
    "\n",
    "        if p_outcomes_given_state is None:\n",
    "            p_outcomes_given_state = np.zeros((nr_of_possible_states, nr_of_possible_outcomes))\n",
    "            p_outcomes_given_state[0, 0] = 1  # Always get nothing in the DR\n",
    "            p_outcomes_given_state[1:nr_of_possible_states] = [\n",
    "                                                                          1 / nr_of_possible_outcomes] * nr_of_possible_outcomes\n",
    "\n",
    "        if starting_states is None:\n",
    "            starting_states = [[1, 0, 0, 0, 0]] * nr_of_agents\n",
    "\n",
    "        self.agent_states = starting_states\n",
    "\n",
    "        self.p_outcomes_given_state = p_outcomes_given_state  # A: Expected outcomes given the state\n",
    "        self.p_future_states_given_control = p_states_given_action_and_state  # B: Expected future state given the current state and the control states(?)\n",
    "        self.actions = [0, 1, 2]\n",
    "\n",
    "    def act(self, agent_i):\n",
    "        \"\"\"\n",
    "        Generate an observation and an outcome for an agent\n",
    "        :param agent_i: Agent index, this is required because the generative process keeps track of the true state of\n",
    "                        every agent separately\n",
    "        :return: An outcome based on the agent's true state\n",
    "        \"\"\"\n",
    "        agent_state = self.agent_states[agent_i]\n",
    "\n",
    "        # Sample an outcome\n",
    "        outcome = np.random.multinomial(1, np.dot(agent_state,\n",
    "                                                  self.p_outcomes_given_state))\n",
    "        return outcome\n",
    "\n",
    "    # Observations\n",
    "    def observe(self, action, agent_i):\n",
    "        \"\"\"\n",
    "        Observe an action made by an agent\n",
    "        :param action: The action made by an agent\n",
    "        :param agent_i: Agent index, this is required because the generative process keeps track of the true state of\n",
    "                        every agent separately\n",
    "        \"\"\"\n",
    "        # Sample agent state\n",
    "        future_state = np.random.multinomial(1, np.dot(self.agent_states[agent_i],\n",
    "                                                       self.p_future_states_given_control[action]))\n",
    "        self.agent_states[agent_i] = future_state  # Update agent state\n",
    "\n",
    "    def create_transition_matrix(self, size=5):\n",
    "        \"\"\"\n",
    "        Create Default state transition matrix\n",
    "        :param size: Number of possible states agent can be in\n",
    "        :return: Default state transition matrix\n",
    "        \"\"\"\n",
    "        transition_matrix = np.zeros((2, size, size))\n",
    "        transition_matrix[0, :, 0] = 0.9\n",
    "        transition_matrix[0, :, 1:size] = 0.025\n",
    "        transition_matrix[1, :, 0] = 0.05\n",
    "        transition_matrix[1, :, 1:size] = 0.95 / (size - 1)\n",
    "        return transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvolutionaryFunction:\n",
    "    \"\"\"\n",
    "    Evolutionary function class, responsible for creating new generations of agents and keeping track of agent data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nr_of_agents=20, agent_nr_of_possible_states=5, utilities_per_state=None,\n",
    "                 agent_expected_outcomes=None,\n",
    "                 agents=None):\n",
    "        \"\"\"\n",
    "        Initialization function of the EvolutionaryFunction\n",
    "        :param nr_of_agents: Number of agents per generation\n",
    "        :param agent_nr_of_possible_states: Number of possible states an agent can be in\n",
    "        :param utilities_per_state: List specifying the evolutionary utility per state\n",
    "        :param agent_expected_outcomes:  List specifying the default expected outcomes of agents\n",
    "        :param agents: List of agents that takes the form of [[agents], [agent utilities]]\n",
    "        \"\"\"\n",
    "        if utilities_per_state is None:\n",
    "            utilities_per_state = [0, 1, 2, 3, 4]  # Default evolutionary utility per state\n",
    "\n",
    "        if agent_expected_outcomes is None:\n",
    "            agent_expected_outcomes = [0.2, 0.2, 0.2, 0.2, 0.2]  # By default an agent does not expect anything\n",
    "\n",
    "        if agents is None:\n",
    "            self.agents = []  # In this case agents should be created using the create_new_generation() function\n",
    "            self.agent_utilities = []\n",
    "        else:\n",
    "            self.agents = agents[0]\n",
    "            self.agent_utilities = agents[1]\n",
    "\n",
    "        self.nr_of_agents = nr_of_agents\n",
    "        self.agent_nr_of_possible_states = agent_nr_of_possible_states\n",
    "        self.agent_actions = []\n",
    "        self.utilities_per_state = utilities_per_state\n",
    "        self.current_generation_states = [0, 0, 0, 0, 0]\n",
    "        self.agent_expected_outcomes = agent_expected_outcomes\n",
    "\n",
    "        # Generational Data collection\n",
    "        # Comments show default values, same values with which simulations were run\n",
    "        self.utility_per_generation = []  # [0: Total comulative, 1: average utlity, 2: most utility]\n",
    "        self.actions_per_generation = []  # [0: Proportion left, 1: Proportion stay, 2: proportion right]\n",
    "        self.expected_outcomes_per_generation = []  # [p(o_0), p(o_1), p(o_2), p(o_3), p(o_4)]\n",
    "        self.surprise_per_generation = []  # [Average surprise per agent per generation]\n",
    "        self.agents_per_generation = []  # [By default always same as nr_of_agents]\n",
    "        self.states_per_generation = []  # [state occupation per agent per generation]\n",
    "\n",
    "    def create_new_generation(self, new_generation=None):\n",
    "        \"\"\"\n",
    "        For creating either a blank-slate generation or setting a new generation with a list of agents\n",
    "        :param new_generation: optional list of agents\n",
    "        \"\"\"\n",
    "        # By default this happens\n",
    "        if new_generation == None:\n",
    "            self.agents = [\n",
    "                GenerativeModel(size=self.agent_nr_of_possible_states, expected_outcomes=self.agent_expected_outcomes)\n",
    "                for i in range(self.nr_of_agents)]  # Create a new blank-slate generation\n",
    "            self.agent_utilities = np.ones(self.nr_of_agents)\n",
    "            self.agent_actions = np.zeros((self.nr_of_agents, 3))  # Assuming 3 possible actions\n",
    "\n",
    "        # If we have already created a new generation (using select_probabilistically() for example)\n",
    "        else:\n",
    "            self.agents = new_generation\n",
    "            self.agent_utilities = np.zeros(len(new_generation))\n",
    "            self.agent_actions = np.zeros((self.nr_of_agents, 3))  # Assuming 3 possible action\n",
    "            self.current_generation_states = [0, 0, 0, 0, 0]\n",
    "\n",
    "    def select_probabilistically(self, N, killoff=None):\n",
    "        \"\"\"\n",
    "        Probabilistically selects parents for a new generation of agents, then creates the agents.\n",
    "        :param N: Number of agents\n",
    "        :param killoff: A number specifying how many of the bottom agents to kill off (0% chance of reproduction)\n",
    "        :return: A new generation of agents\n",
    "        \"\"\"\n",
    "        agent_utilities = np.array(self.agent_utilities)  # For normal evolutionary pressure\n",
    "        total_utility = np.sum(agent_utilities)\n",
    "        p = agent_utilities / total_utility\n",
    "\n",
    "        # if killoff is None:  # Uncomment this for heightened evolutionary pressure\n",
    "        #     killoff = round(N / 10)  # Kill off the bottom (least evolutionary utility) 10% of agents\n",
    "        # eligible_agents = np.argsort(agent_utilities)[killoff:]  # Select agents which may reproduce\n",
    "        # total_utility = np.sum(agent_utilities[eligible_agents])\n",
    "        # p = np.zeros(N)\n",
    "        # p[eligible_agents] = agent_utilities[eligible_agents] / total_utility  # Specify the probabilities of\n",
    "        # # reproducing for each agent\n",
    "\n",
    "\n",
    "        new_generation = []\n",
    "        parents = np.random.choice(a=np.arange(N), size=N, p=p)  # Select parents\n",
    "        # parents = np.append(parents, np.argmax(agent_utilities))  # Optional: Best performing agent always reproduces\n",
    "\n",
    "        for p in parents:\n",
    "            # Mutate Prior over outcomes given state\n",
    "            new_predicted_outcomes_given_state = self.mutate_predicted_outcomes_given_state(\n",
    "                self.agents[p].starting_A_alphas) + \\\n",
    "                                                 self.agents[p].A_alphas / \\\n",
    "                                                 np.sum(self.agents[p].A_alphas, axis=1)[\n",
    "                                                     ..., None] * 5 # Normalize and multiply by 5 so that ending alphas have limited influence on the starting alphas\n",
    "\n",
    "            # Mutate Prior over state given action and state\n",
    "            new_state_transition_function = self.mutate_state_transition_function(self.agents[p].starting_B_alphas) + \\\n",
    "                                            self.agents[p].B_alphas / \\\n",
    "                                            np.sum(self.agents[p].B_alphas, axis=2)[\n",
    "                                                ..., None] * 5  # Normalize and multiply by 5 so that ending alphas have limited influence on the starting alphas\n",
    "\n",
    "            # Mutate Expected outcomes\n",
    "            new_expected_outcomes = self.mutate_expected_outcomes(self.agents[p].expected_outcomes)\n",
    "\n",
    "            # Add child to the new generation\n",
    "            new_generation += [\n",
    "                GenerativeModel(expected_outcomes=new_expected_outcomes,\n",
    "                                starting_B_alphas=new_state_transition_function,\n",
    "                                starting_A_alphas=new_predicted_outcomes_given_state)]\n",
    "        return new_generation\n",
    "\n",
    "    def mutate_predicted_outcomes_given_state(self, predicted_outcomes_given_state, mu=0, sigma=0.5):\n",
    "        \"\"\"\n",
    "        Mutates the given list of predicted outcomes per state by adding a sample from a normal distribution\n",
    "        to the probability of each state, then normalizing.\n",
    "        :param predicted_outcomes_given_state: Must be a 2-dimensional list or ndarray\n",
    "        :param sigma: standard deviation\n",
    "        :param mu: mean\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        predicted_outcomes_given_state = np.array(predicted_outcomes_given_state)\n",
    "        new_predicted_outcomes_given_state = predicted_outcomes_given_state + np.random.normal(mu, sigma,\n",
    "                                                                                               predicted_outcomes_given_state.shape)\n",
    "        new_predicted_outcomes_given_state[\n",
    "            new_predicted_outcomes_given_state <= 0] = 0.001  # Alphas must be larger than 0\n",
    "        new_predicted_outcomes_given_state = new_predicted_outcomes_given_state / \\\n",
    "                                             np.sum(new_predicted_outcomes_given_state, axis=1)[\n",
    "                                                 ..., None] * 5  # Multiple by 5 and normalize so that alpha values do not grow too large\n",
    "        return new_predicted_outcomes_given_state\n",
    "\n",
    "    def mutate_state_transition_function(self, state_transition_function, mu=0, sigma=0.5):\n",
    "        \"\"\"\n",
    "        Mutates the given list of predicted states per action and state by adding a sample from a normal distribution\n",
    "        to the probability of each state, then normalizing.\n",
    "        :param state_transition_function: Must be a 3-dimensional list or ndarray\n",
    "        :param sigma: standard deviation\n",
    "        :param mu: mean\n",
    "        :return: Mutated version of the state transition function\n",
    "        \"\"\"\n",
    "        state_transition_function = np.array(state_transition_function)\n",
    "        new_state_transition_function = state_transition_function + np.random.normal(mu, sigma,\n",
    "                                                                                     state_transition_function.shape)\n",
    "        new_state_transition_function[new_state_transition_function <= 0] = 0.001  # Alphas must be larger than 0\n",
    "        new_state_transition_function = new_state_transition_function / \\\n",
    "                                        np.sum(new_state_transition_function, axis=2)[\n",
    "                                            ..., None] * 5  # Multiple by 5 and normalize so that alpha values do not grow too large\n",
    "\n",
    "        return new_state_transition_function\n",
    "\n",
    "    def mutate_expected_outcomes(self, expected_outcomes, mu=0, sigma=0.02):\n",
    "        \"\"\"\n",
    "        Mutates the given list of expected outcomes by adding a sample from a normal distribution to the probability\n",
    "        of each outcome, then normalizing.\n",
    "        :param expected_outcomes: Must be a 1-dimensional list or ndarray\n",
    "        :param sigma: standard deviation\n",
    "        :param mu: mean\n",
    "        :return: Mutated version of expected outcomes\n",
    "        \"\"\"\n",
    "        expected_outcomes = np.array(expected_outcomes)\n",
    "        new_expected_outcomes = np.clip(expected_outcomes + np.random.normal(mu, sigma, expected_outcomes.shape), 0.001,\n",
    "                                        1)\n",
    "        norm = np.sum(new_expected_outcomes)  # Normalization\n",
    "        new_expected_outcomes /= norm\n",
    "        return new_expected_outcomes\n",
    "\n",
    "    def evolutionary_utility(self, outcome):\n",
    "        \"\"\"\n",
    "        Returns the utility of a certain outcome\n",
    "        :param outcome: array specifying outcome(s)\n",
    "        :return: evolutionary utility of outcomes\n",
    "        \"\"\"\n",
    "        utility = np.dot(self.utilities_per_state, outcome)\n",
    "        return utility\n",
    "\n",
    "    def store_generation_data(self):\n",
    "        \"\"\"\n",
    "        Store relevant data of this generation for plotting purposes\n",
    "        \"\"\"\n",
    "        total_utility = np.sum(self.agent_utilities)\n",
    "        average_utility = total_utility / len(self.agent_utilities)\n",
    "        most_utility = np.max(self.agent_utilities)\n",
    "        self.utility_per_generation.append([total_utility, average_utility, most_utility])\n",
    "        self.actions_per_generation.append(np.sum(self.agent_actions, axis=0) / np.sum(\n",
    "            self.agent_actions))  # Only store the average proportion of actions\n",
    "        self.expected_outcomes_per_generation.append(\n",
    "            self.get_expected_outcomes())  # Only store the average expected outcomes\n",
    "        self.agents_per_generation.append(\n",
    "            self.nr_of_agents)  # Normally this should always be the same, may not be for different ways of generation creation\n",
    "\n",
    "    def get_expected_outcomes(self):\n",
    "        \"\"\"\n",
    "        Get the average expected outcomes of the current generation\n",
    "        :return: Average expected outcomes per agent\n",
    "        \"\"\"\n",
    "        expected_outcomes = np.zeros((self.nr_of_agents, self.agents[0].expected_outcomes.shape[0]))\n",
    "        for i, a in enumerate(self.agents):\n",
    "            expected_outcomes[i] = a.expected_outcomes\n",
    "        return expected_outcomes\n",
    "\n",
    "    def get_avg_expected_outcomes(self):\n",
    "        \"\"\"\n",
    "        Get the average expected outcomes of the current generation\n",
    "        :return: Average expected outcomes per agent\n",
    "        \"\"\"\n",
    "        average_expected_outcomes = np.zeros(self.agent_nr_of_possible_states)\n",
    "        for a in self.agents:\n",
    "            average_expected_outcomes += a.expected_outcomes\n",
    "        average_expected_outcomes /= len(self.agents)\n",
    "        return average_expected_outcomes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Evolution_sandbox(T=20, G=2, A=1, size=5, GP=None, EF=None, cont=False):\n",
    "    \"\"\"\n",
    "    This function runs the evolution simulation\n",
    "    :param T: Number of timesteps per generation\n",
    "    :param G: Number of generations\n",
    "    :param A: Number of agents per generation\n",
    "    :param size: Number of possible states the agent can be in\n",
    "    :param GP: Generative Process\n",
    "    :param EF: Evolutionary Function\n",
    "    :param cont: If this is true, the simulation will continue with the agents the Evolutionary Function already has,\n",
    "                    otherwise a new simulation is run with a blank-slate first generation\n",
    "    :return: The Evolutionary Function, the Evolutionary Function contains relevant data\n",
    "    \"\"\"\n",
    "    if GP is None:\n",
    "        GP = GenerativeProcess(nr_of_possible_states=size, nr_of_agents=A)\n",
    "    if EF is None:\n",
    "        EF = EvolutionaryFunction(nr_of_agents=A)\n",
    "\n",
    "    for g in range(G):\n",
    "        # print(g)\n",
    "        agent_states_this_generation = []\n",
    "        true_agent_states_this_generation = []\n",
    "\n",
    "        # The first generation, determine if we start from scratch or continue with the agents the\n",
    "        # EvolutionaryFunction already has.\n",
    "        if g == 0 and not cont:\n",
    "            EF.create_new_generation()\n",
    "\n",
    "        EF.create_new_generation(EF.select_probabilistically(N=A))\n",
    "        starting_states = generate_starting_locations(EF.nr_of_agents, size)\n",
    "        GP.agent_states = starting_states\n",
    "        true_agent_states_this_generation.append(np.sum(starting_states, axis=0) / EF.nr_of_agents)\n",
    "        surprise_this_generation = []\n",
    "\n",
    "        for t in range(T):\n",
    "            agent_states_this_timestep = [0, 0, 0, 0, 0]\n",
    "            true_agent_states_this_timestep = [0, 0, 0, 0, 0]\n",
    "            surprise_this_timestep = []\n",
    "            for a in range(EF.nr_of_agents):\n",
    "                # Agent acts, environment 'observes'\n",
    "                agent_action = EF.agents[a].act()\n",
    "                EF.agent_actions[a][agent_action] += 1\n",
    "                GP.observe(agent_action, a)\n",
    "\n",
    "                # Environments 'acts', agent observes\n",
    "                outcome = GP.act(a)\n",
    "                inferred_state, surprise = EF.agents[a].observe(outcome)\n",
    "                EF.agent_utilities[a] += [EF.evolutionary_utility(outcome)]\n",
    "\n",
    "                # Collect timestep data\n",
    "                agent_states_this_timestep += inferred_state\n",
    "                true_agent_states_this_timestep += GP.agent_states[a]\n",
    "                surprise_this_timestep += [surprise]\n",
    "            surprise_this_generation += [surprise_this_timestep]\n",
    "\n",
    "            # Collect data generational data\n",
    "            agent_states_this_generation.append(np.array(agent_states_this_timestep).T / EF.nr_of_agents)\n",
    "            true_agent_states_this_generation.append(np.array(true_agent_states_this_timestep).T / EF.nr_of_agents)\n",
    "\n",
    "        EF.surprise_per_generation += [surprise_this_generation]\n",
    "        EF.states_per_generation.append(true_agent_states_this_generation)\n",
    "        EF.store_generation_data()\n",
    "\n",
    "        # For plotting (Optional)\n",
    "\n",
    "        # Plot surprise this generation\n",
    "        plt.figure()\n",
    "        plt.plot(np.mean(surprise_this_generation, axis=1))\n",
    "        plt.title('Average surprise of generation over time ' + str(g))\n",
    "        plt.ylim(-0.1, 4)\n",
    "        plt.xlabel('Timestep')\n",
    "        plt.ylabel('Surprise')\n",
    "        # plt.savefig('Surprise' + str(g))\n",
    "        plt.show()\n",
    "\n",
    "        # Plot inferred agent states of this generation\n",
    "        # take_X_timestep_average(10, agent_states_this_generation, 'Average inferred agent states of generation ' + str(g), 'State probability', 'Timestep')\n",
    "        plt.figure()\n",
    "        plt.plot(agent_states_this_generation)\n",
    "        plt.title('Average inferred state of generation ' + str(g))\n",
    "        plt.legend(['0', '1', '2', '3', '4'])\n",
    "        plt.ylim(-0.1, 1.2)\n",
    "        plt.xlabel('Timestep')\n",
    "        plt.ylabel('state probability')\n",
    "        # plt.savefig('Test inferred states' + str(g))\n",
    "        plt.show()\n",
    "\n",
    "        # Plot true agent states of this generation\n",
    "        #         take_X_timestep_average(10, true_agent_states_this_generation, 'Average true state of generation ' + str(g) , 'state', 'Timestep')\n",
    "        plt.figure()\n",
    "        plt.plot(true_agent_states_this_generation)\n",
    "        plt.title('Average true state of generation ' + str(g))\n",
    "        plt.legend(['0', '1', '2', '3', '4'])\n",
    "        plt.ylim(-0.1, 1.2)\n",
    "        plt.xlabel('Timestep')\n",
    "        plt.ylabel('state')\n",
    "        # plt.savefig('Test true states' + str(g))\n",
    "        plt.show()\n",
    "    return EF\n",
    "\n",
    "\n",
    "def generate_starting_locations(nr_of_agents, nr_of_possible_states, p=None):\n",
    "    \"\"\"\n",
    "    Generate semi-random starting locations for a number of agents\n",
    "    :param nr_of_agents: Number of agents\n",
    "    :param nr_of_possible_states: Number of possible states an agent can be in\n",
    "    :param p: Probability of each state\n",
    "    :return: A list of starting locations\n",
    "    \"\"\"\n",
    "    if p is None:\n",
    "        p = np.ones(nr_of_possible_states) / nr_of_possible_states\n",
    "    starting_states = []\n",
    "    for a in range(nr_of_agents):\n",
    "        starting_states.append(np.random.multinomial(1, p))\n",
    "    return starting_states\n",
    "\n",
    "\n",
    "def single_agent(T=1000, GP=None, agent=None, utility=None):\n",
    "    \"\"\"\n",
    "    Function for examining the behaviour of a singular agent\n",
    "    :param T: Number of timesteps the agent will 'live'\n",
    "    :param GP: Generative Process\n",
    "    :param agent: The agent\n",
    "    :param utility: Utility per outcome\n",
    "    :return: The agent\n",
    "    \"\"\"\n",
    "    if GP is None:\n",
    "        GP = GenerativeProcess(nr_of_agents=1)\n",
    "    if agent is None:\n",
    "        agent = GenerativeModel()\n",
    "    if utility is None:\n",
    "        utility = [0, 2, 4, 6, 8]\n",
    "\n",
    "    starting_states = generate_starting_locations(1, 5)\n",
    "    GP.agent_states = [starting_states[0]]\n",
    "    agent_state = []\n",
    "    agent_actions = []\n",
    "    agent_utility = []\n",
    "    true_agent_state = [starting_states[0]]\n",
    "    observed_outcomes = []\n",
    "    agent_surprise = []\n",
    "    for t in range(T):\n",
    "        # Agent acts, environment 'observes'\n",
    "        agent_action = agent.act()\n",
    "        GP.observe(agent_action, 0)\n",
    "\n",
    "        # Environments 'acts', agent observes\n",
    "        outcome = GP.act(0)\n",
    "        inferred_state, surprise = agent.observe(outcome)\n",
    "\n",
    "        agent_utility += [[np.dot(utility, outcome)]]\n",
    "\n",
    "        x = np.zeros(3)\n",
    "        x[agent_action] = 1\n",
    "        agent_actions.append(x)\n",
    "        agent_state.append(inferred_state)\n",
    "        observed_outcomes.append(outcome)\n",
    "        agent_surprise.append([surprise])\n",
    "        true_agent_state.append(GP.agent_states[0])\n",
    "\n",
    "    avg_t = 1000\n",
    "    take_X_timestep_average(avg_t, agent_surprise, str(avg_t) + ' Timestep average surprise', 'Timestep', 'Surprise',\n",
    "                            ylim=(-0.1, 2), legend=['agent utility'])\n",
    "    take_X_timestep_average(avg_t, agent_utility, str(avg_t) + ' Timestep average utility', 'Timestep', 'Utility',\n",
    "                            ylim=(-0.1, 20), legend=['agent utility'])\n",
    "    take_X_timestep_average(avg_t, agent_actions, str(avg_t) + ' Timestep average agent action', 'Timestep',\n",
    "                            'Action proportion', legend=['left', 'stay', 'right'])\n",
    "    take_X_timestep_average(avg_t, agent_state, str(avg_t) + ' Timestep average inferred agent state', 'Timestep',\n",
    "                            'State proportion')\n",
    "    take_X_timestep_average(avg_t, true_agent_state, str(avg_t) + ' Timestep average true agent state', 'Timestep',\n",
    "                            'State proportion')\n",
    "    take_X_timestep_average(avg_t, observed_outcomes, str(avg_t) + ' Timestep average observed outcome', 'Timestep',\n",
    "                            'Observation proportion',\n",
    "                            legend=['outcome 0', 'outcome 1', 'outcome 2', 'outcome 3', 'outcome 4'])\n",
    "    return agent\n",
    "\n",
    "\n",
    "def take_X_timestep_average(X, values, title=None, xlabel=None, ylabel=None, ylim=None, legend=None):\n",
    "    \"\"\"\n",
    "    Plot a sliding-window timestep average of some list of values\n",
    "    :param X: Size of the window\n",
    "    :param values: Some list of values\n",
    "    :param title: Title of the plot\n",
    "    :param xlabel: xlabel of the plot\n",
    "    :param ylabel: ylabel of the plot\n",
    "    :param ylim: ylim of the plot\n",
    "    :param legend: legend of the plot\n",
    "    :return: Returns the list of averages\n",
    "    \"\"\"\n",
    "    if ylabel is None:\n",
    "        ylabel = 'Y_value'\n",
    "    if xlabel is None:\n",
    "        xlabel = 'Timestep'\n",
    "    if title is None:\n",
    "        title = 'plot'\n",
    "    if ylim is None:\n",
    "        ylim = (-0.1, 1.2)\n",
    "    if legend is None:\n",
    "        legend = ['state 0', 'state 1', 'state 2', 'state 3', 'state 4']\n",
    "    values = np.array(values)\n",
    "    values = np.insert(values, 0, np.zeros([X, values.shape[1]]), axis=0)\n",
    "    averages = []\n",
    "    for t in range(values.shape[0]):\n",
    "        averages.append(np.sum(values[t - X:t], axis=0) / X)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(averages[X:])\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylim(ylim[0], ylim[1])\n",
    "    plt.legend(legend)\n",
    "    plt.savefig('THESIS_not_modified ' + str(title))\n",
    "    plt.show()\n",
    "    return averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_simulation():\n",
    "    \"\"\"\n",
    "    Runs an evolution simulation,\n",
    "    Change parameters in this function to adjust the simulation\n",
    "    :return: Evolutionary Function\n",
    "    \"\"\"\n",
    "    utilities = [1, 5, 25, 125, 625]  # Evolutionary utility of each outcome\n",
    "\n",
    "    # P_gp(O|S)\n",
    "    predicted_outcomes_given_state = np.array([[1, 0.0, 0.0, 0.0, 0.0],\n",
    "                                               [0.05, 0.8, 0.05, 0.05, 0.05],\n",
    "                                               [0.05, 0.05, 0.8, 0.05, 0.05],\n",
    "                                               [0.05, 0.05, 0.05, 0.8, 0.05],\n",
    "                                               [0.05, 0.05, 0.05, 0.05, 0.8]])\n",
    "\n",
    "    # P_gp(S_{t+1}|A_t,S_t)\n",
    "    predicted_future_states_given_control = np.array([[[0.1, 0.1, 0, 0, 0.8],\n",
    "                                                       [0.8, 0.1, 0.1, 0, 0],\n",
    "                                                       [0, 0.8, 0.1, 0.1, 0],\n",
    "                                                       [0, 0, 0.8, 0.1, 0.1],\n",
    "                                                       [0.1, 0, 0, 0.8, 0.1]],\n",
    "\n",
    "                                                      [[0.9, 0.05, 0, 0, 0.05],\n",
    "                                                       [0.05, 0.9, 0.05, 0, 0],\n",
    "                                                       [0, 0.05, 0.9, 0.05, 0],\n",
    "                                                       [0, 0, 0.05, 0.9, 0.05],\n",
    "                                                       [0.05, 0, 0, 0.05, 0.9]],\n",
    "\n",
    "                                                      [[0.1, 0.8, 0, 0, 0.1],\n",
    "                                                       [0.1, 0.1, 0.8, 0, 0],\n",
    "                                                       [0, 0.1, 0.1, 0.8, 0],\n",
    "                                                       [0, 0, 0.1, 0.1, 0.8],\n",
    "                                                       [0.8, 0, 0, 0.1, 0.1]]])\n",
    "\n",
    "    T = 1825  # Number of timesteps each agent lives\n",
    "    G = 100  # Number of generations\n",
    "    A = 100  # Number of agetns per generation\n",
    "\n",
    "    GP = GenerativeProcess(nr_of_agents=A, p_states_given_action_and_state=predicted_future_states_given_control,\n",
    "                           p_outcomes_given_state=predicted_outcomes_given_state)\n",
    "    EF = EvolutionaryFunction(nr_of_agents=A, utilities_per_state=utilities)\n",
    "    EF = Evolution_sandbox(T=T, G=G, A=A, GP=GP, EF=EF)\n",
    "    return EF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EF = run_simulation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_agent():\n",
    "    \"\"\"\n",
    "    Runs a single agent simulation,\n",
    "    Change parameters in this function to adjust the simulation\n",
    "    :return: The agent after simulation\n",
    "    \"\"\"\n",
    "    # P_gp(O|S)\n",
    "    predicted_outcomes_given_state = np.array([[1, 0.0, 0.0, 0.0, 0.0],\n",
    "                                               [0.05, 0.8, 0.05, 0.05, 0.05],\n",
    "                                               [0.05, 0.05, 0.8, 0.05, 0.05],\n",
    "                                               [0.05, 0.05, 0.05, 0.8, 0.05],\n",
    "                                               [0.05, 0.05, 0.05, 0.05, 0.8]])\n",
    "\n",
    "    # P_gp(S_{t+1}|A_t,S_t)\n",
    "    predicted_future_states_given_control = np.array([[[0.1, 0.1, 0, 0, 0.8],\n",
    "                                                       [0.8, 0.1, 0.1, 0, 0],\n",
    "                                                       [0, 0.8, 0.1, 0.1, 0],\n",
    "                                                       [0, 0, 0.8, 0.1, 0.1],\n",
    "                                                       [0.1, 0, 0, 0.8, 0.1]],\n",
    "\n",
    "                                                      [[0.9, 0.05, 0, 0, 0.05],\n",
    "                                                       [0.05, 0.9, 0.05, 0, 0],\n",
    "                                                       [0, 0.05, 0.9, 0.05, 0],\n",
    "                                                       [0, 0, 0.05, 0.9, 0.05],\n",
    "                                                       [0.05, 0, 0, 0.05, 0.9]],\n",
    "\n",
    "                                                      [[0.1, 0.8, 0, 0, 0.1],\n",
    "                                                       [0.1, 0.1, 0.8, 0, 0],\n",
    "                                                       [0, 0.1, 0.1, 0.8, 0],\n",
    "                                                       [0, 0, 0.1, 0.1, 0.8],\n",
    "                                                       [0.8, 0, 0, 0.1, 0.1]]])\n",
    "    A = 1\n",
    "\n",
    "    GP = GenerativeProcess(nr_of_agents=A, p_states_given_action_and_state=predicted_future_states_given_control,\n",
    "                           p_outcomes_given_state=predicted_outcomes_given_state)\n",
    "\n",
    "    agent_A_alphas = (predicted_outcomes_given_state + 0.001)  # Starting A alphas of the agent\n",
    "    agent_B_alphas = (predicted_future_states_given_control + 0.001)  # Starting B alphas of the agent\n",
    "\n",
    "    expected_outcomes = [0.2, 0.2, 0.2, 0.2, 0.2]  # Expected outcomes of the agent\n",
    "    agent = GenerativeModel(starting_A_alphas=agent_A_alphas, starting_B_alphas=agent_B_alphas,\n",
    "                            expected_outcomes=expected_outcomes)\n",
    "\n",
    "    agent = single_agent(T=10000, GP=GP, agent=agent)\n",
    "    return agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = run_single_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://stackoverflow.com/questions/15033511/compute-a-confidence-interval-from-sample-data\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Creates confidence intervals for the expected outcomes per generation.\n",
    "    :data: 3-dimensional data with shape [G, A, O]\n",
    "            G is the number of generations\n",
    "            A is the number of agents per generations\n",
    "            O is the number of possible outcomes\n",
    "    :return: the means of the data,\n",
    "                the means plus the positive edge of the confidence interval,\n",
    "                and the means plus the negative edge of the confidence interval\n",
    "    \"\"\"\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = a.shape[1]\n",
    "    m, se = np.mean(a, axis=1), scipy.stats.sem(a, axis=1)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n - 1)\n",
    "    return m, m - h, m + h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(EF, T=1000):\n",
    "    \"\"\"\n",
    "    Plots the average expected outcomes per generation,\n",
    "            the average surprise per generation,\n",
    "            the average proportion of actions per generation,\n",
    "            and the average utility per timestep per generation\n",
    "    :param EF: Evolutionary Function\n",
    "    :param T: Timesteps per generation\n",
    "    \"\"\"\n",
    "    y = np.array(EF.expected_outcomes_per_generation)\n",
    "    n = y.shape[0]\n",
    "    x = range(n)\n",
    "\n",
    "    plt.plot(x, np.mean(y, axis=1))\n",
    "    plt.title(\"Expected outcomes per generation\")\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Outcome probability')\n",
    "    plt.legend(['Outcome 0', 'Outcome 1', 'Outcome 2', 'Outcome 3', 'Outcome 4'])\n",
    "    #     plt.savefig('Expected_outcomes_per_generation')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(np.mean(np.mean(EF.surprise_per_generation, axis=1), axis=1))\n",
    "    plt.title(\"Surprise per generation\")\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Surprise')\n",
    "    #     plt.savefig('Surprise_per_generation')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(np.array(EF.actions_per_generation)[:, 0])\n",
    "    plt.plot(np.array(EF.actions_per_generation)[:, 1])\n",
    "    plt.plot(np.array(EF.actions_per_generation)[:, 2])\n",
    "    plt.legend(['Left', 'Stay', 'Right'])\n",
    "    plt.title('Proportion of actions per generation')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Action proportion')\n",
    "    #     plt.savefig('Agent_actions')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(np.array(EF.utility_per_generation)[:, 1] / T)\n",
    "    plt.title('Average utility per timestep per generation')\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Average utility')\n",
    "    #     plt.savefig('Average_utility')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_expected_outcomes(EF):\n",
    "    \"\"\"\n",
    "    Plots the average expected outcomes per generation with (by default) 95% confidence intervals\n",
    "    :param EF: Evolutionary function\n",
    "    \"\"\"\n",
    "    y = np.array(EF.expected_outcomes_per_generation)\n",
    "    n = y.shape[0]\n",
    "    x = range(n)\n",
    "\n",
    "    plt.plot(x, np.mean(y, axis=1))\n",
    "    plt.plot(x, np.ones(n) * 0.2, color='black')\n",
    "    # Outcome 0\n",
    "    mean, lower, upper = mean_confidence_interval(y[:, :, 0])\n",
    "    ci_m = np.mean(y[:, :, 0], axis=1)\n",
    "    plt.fill_between(x, ci_m - lower, ci_m + upper, alpha=.1)\n",
    "\n",
    "    mean, lower, upper = mean_confidence_interval(y[:, :, 1])\n",
    "    ci_m = np.mean(y[:, :, 1], axis=1)\n",
    "    plt.fill_between(x, ci_m - lower, ci_m + upper, alpha=.1)\n",
    "\n",
    "    mean, lower, upper = mean_confidence_interval(y[:, :, 2])\n",
    "    ci_m = np.mean(y[:, :, 2], axis=1)\n",
    "    plt.fill_between(x, ci_m - lower, ci_m + upper, alpha=.1)\n",
    "\n",
    "    mean, lower, upper = mean_confidence_interval(y[:, :, 3])\n",
    "    ci_m = np.mean(y[:, :, 3], axis=1)\n",
    "    plt.fill_between(x, ci_m - lower, ci_m + upper, alpha=.1)\n",
    "\n",
    "    mean, lower, upper = mean_confidence_interval(y[:, :, 4])\n",
    "    ci_m = np.mean(y[:, :, 4], axis=1)\n",
    "    plt.fill_between(x, ci_m - lower, ci_m + upper, alpha=.1)\n",
    "\n",
    "    plt.title(\"Expected outcomes per generation\")\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Outcome probability')\n",
    "    plt.legend(['Outcome 0', 'Outcome 1', 'Outcome 2', 'Outcome 3', 'Outcome 4', 'baseline'])\n",
    "    #     plt.savefig('Confidence_intervals')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_expected_outcomes_seperate(EF):\n",
    "    \"\"\"\n",
    "    Plots the average expected outcomes per generation with (by default) 95% confidence intervals,\n",
    "    each outcome is plotted separately.\n",
    "    :param EF: Evolutionary Function\n",
    "    \"\"\"\n",
    "    y = np.array(EF.expected_outcomes_per_generation)\n",
    "    n = y.shape[0]\n",
    "    x = range(n)\n",
    "\n",
    "    # Outcome 0\n",
    "    plt.plot(x, np.mean(y[:, :, 0], axis=1))\n",
    "    plt.plot(x, np.ones(n) * 0.2, color='black')\n",
    "    mean, lower, upper = mean_confidence_interval(y[:, :, 0])\n",
    "    ci_m = np.mean(y[:, :, 0], axis=1)\n",
    "    plt.fill_between(x, ci_m - lower, ci_m + upper, alpha=.1)\n",
    "    plt.title(\"Expected outcomes per generation\")\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Outcome probability')\n",
    "    plt.legend(['Outcome 0'])\n",
    "    plt.ylim([0, 1])\n",
    "    # plt.savefig('Confidence_interval_outcome_0')\n",
    "    plt.show()\n",
    "\n",
    "    # Outcome 1\n",
    "    plt.plot(x, np.mean(y[:, :, 1], axis=1), color='orange')\n",
    "    plt.plot(x, np.ones(n) * 0.2, color='black')\n",
    "    mean, lower, upper = mean_confidence_interval(y[:, :, 1])\n",
    "    ci_m = np.mean(y[:, :, 1], axis=1)\n",
    "    plt.fill_between(x, ci_m - lower, ci_m + upper, color='orange', alpha=.1)\n",
    "    plt.title(\"Expected outcomes per generation\")\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Outcome probability')\n",
    "    plt.legend(['Outcome 1'])\n",
    "    plt.ylim([0, 1])\n",
    "    # plt.savefig('Confidence_interval_outcome_1')\n",
    "    plt.show()\n",
    "\n",
    "    # Outcome 2\n",
    "    plt.plot(x, np.mean(y[:, :, 2], axis=1), color='green')\n",
    "    plt.plot(x, np.ones(n) * 0.2, color='black')\n",
    "    mean, lower, upper = mean_confidence_interval(y[:, :, 2])\n",
    "    ci_m = np.mean(y[:, :, 2], axis=1)\n",
    "    plt.fill_between(x, ci_m - lower, ci_m + upper, color='green', alpha=.1)\n",
    "    plt.title(\"Expected outcomes per generation\")\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Outcome probability')\n",
    "    plt.legend(['Outcome 2'])\n",
    "    plt.ylim([0, 1])\n",
    "    # plt.savefig('Confidence_interval_outcome_2')\n",
    "    plt.show()\n",
    "\n",
    "    # Outcome 3\n",
    "    plt.plot(x, np.mean(y[:, :, 3], axis=1), color='red')\n",
    "    plt.plot(x, np.ones(n) * 0.2, color='black')\n",
    "    mean, lower, upper = mean_confidence_interval(y[:, :, 3])\n",
    "    ci_m = np.mean(y[:, :, 3], axis=1)\n",
    "    plt.fill_between(x, ci_m - lower, ci_m + upper, color='red', alpha=.1)\n",
    "    plt.title(\"Expected outcomes per generation\")\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Outcome probability')\n",
    "    plt.legend(['Outcome 3'])\n",
    "    plt.ylim([0, 1])\n",
    "    # plt.savefig('Confidence_interval_outcome_3')\n",
    "    plt.show()\n",
    "\n",
    "    # Outcome 4\n",
    "    plt.plot(x, np.mean(y[:, :, 4], axis=1), color='purple')\n",
    "    plt.plot(x, np.ones(n) * 0.2, color='black')\n",
    "    mean, lower, upper = mean_confidence_interval(y[:, :, 4])\n",
    "    ci_m = np.mean(y[:, :, 4], axis=1)\n",
    "    plt.fill_between(x, ci_m - lower, ci_m + upper, color='purple', alpha=.1)\n",
    "    plt.title(\"Expected outcomes per generation\")\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Outcome probability')\n",
    "    plt.legend(['Outcome 4'])\n",
    "    plt.ylim([0, 1])\n",
    "    # plt.savefig('Confidence_interval_outcome_4')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
